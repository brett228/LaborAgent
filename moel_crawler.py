# moel_fastcounsel_detail_scrape_final_v5.py

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import json
import time
import re

# í¬ë¡¤ë§í•  ê¸°ë³¸ URL
BASE_LIST_URL = "https://www.moel.go.kr/minwon/fastcounsel/fastcounselList.do"

# ìš”ì²­ í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36",
}

def parse_detail(page):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©ê³¼ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    title = ""
    try:
        # ì œëª© íŒŒì‹±
        title_selectors = ["h3", ".board_view_title", ".title", ".tit", ".view_tit"] 
        for selector in title_selectors:
            title_el = page.query_selector(selector)
            if title_el:
                title = title_el.inner_text().strip()
                break
                
        # ë‚´ìš© íŒŒì‹±
        content_selectors = [".board_view_content", ".content_view", "article", ".view_content", "div.article"]
        content = ""
        for selector in content_selectors:
            content_el = page.query_selector(selector)
            if content_el:
                content = content_el.inner_text().strip()
                break
                
        # ë‚´ìš© í™•ì¸ ë° ì—ëŸ¬ ë©”ì‹œì§€ ì²˜ë¦¬
        if not content:
            body_text = page.inner_text("body") 
            if "ì£„ì†¡í•©ë‹ˆë‹¤" in body_text or "ìš”ì²­ í•˜ì…¨ìŠµë‹ˆë‹¤" in body_text:
                content = "[ERROR] ì ‘ê·¼ ë¶ˆê°€ í˜ì´ì§€ë¡œ í™•ì¸ë¨"
            else:
                 content = "[WARN] ìƒì„¸ ë‚´ìš© íŒŒì‹± ì‹¤íŒ¨ (ì…€ë ‰í„° ë¯¸ì¼ì¹˜)"
                 
        return {"title": title, "content": content}
        
    except Exception as e:
        return {"title": title, "content": f"[ERROR] ìƒì„¸ íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}"}


def crawl(max_pages=3, delay=1.5, output_json="fastcounsel_with_detail.json"):
    collected = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent=HEADERS["User-Agent"], locale="ko-KR")
        list_page = context.new_page()

        print(f"ğŸš€ {BASE_LIST_URL} ì ‘ì† ì‹œë„...")
        list_page.goto(BASE_LIST_URL, timeout=45000) 
        time.sleep(delay)

        for page_index in range(1, max_pages + 1):
            print(f"\n[info] ğŸ“‹ ì²˜ë¦¬ì¤‘ ëª©ë¡ í˜ì´ì§€ {page_index}")
            
            list_page.goto(f"{BASE_LIST_URL}?pageIndex={page_index}", timeout=45000)
            time.sleep(delay)

            try:
                list_page.wait_for_selector("table tbody tr", timeout=15000)
            except PWTimeout:
                print("[warn] ëª©ë¡ì´ ë¡œë“œë˜ì§€ ì•Šì•„ í•´ë‹¹ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°.")
                continue
            
            # 1. í•­ëª© ì •ë³´ ë¯¸ë¦¬ ì¶”ì¶œ
            rows = list_page.query_selector_all("table tbody tr")
            items_to_process = []
            for tr in rows:
                a = tr.query_selector("td:nth-child(2) a")
                date_td = tr.query_selector("td:nth-child(4)")
                
                if a:
                    items_to_process.append({
                        "title": a.inner_text().strip(),
                        "date": date_td.inner_text().strip() if date_td else "ë¯¸í™•ì¸",
                    })
            
            # 2. ì¶”ì¶œëœ ì •ë³´ë¥¼ ìˆœíšŒí•˜ë©° ìƒì„¸ í¬ë¡¤ë§
            for item in items_to_process:
                title = item['title']
                date = item['date']
                href = "ì¸í˜ì´ì§€ í´ë¦­ ì‹¤íŒ¨"
                
                print(f" Â + ìƒì„¸ ìˆ˜ì§‘ ì‹œë„ (í´ë¦­): {title}")

                try:
                    # 1. ëª©ë¡ í˜ì´ì§€ ì¬ì ‘ì† (DOM ì»¨í…ìŠ¤íŠ¸ ë³µì›)
                    list_page.goto(f"{BASE_LIST_URL}?pageIndex={page_index}", timeout=30000)
                    time.sleep(1.0) 

                    # 2. í•´ë‹¹ A íƒœê·¸ë¥¼ ì œëª© í…ìŠ¤íŠ¸ë¡œ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤.
                    a_locator = list_page.locator("td:nth-child(2) a", has_text=title).first
                    
                    if not a_locator.is_visible():
                        print(f" Â  Â [warn] ë§í¬ ìš”ì†Œë¥¼ ë‹¤ì‹œ ì°¾ëŠ” ë° ì‹¤íŒ¨: {title}")
                        continue
                        
                    # â­â­â­ 3. í´ë¦­ ë° ê°•ì œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ìš°íšŒ) â­â­â­
                    a_locator.click(timeout=10000)
                    
                    print(" Â  Â [debug] 5ì´ˆê°„ ê°•ì œ ëŒ€ê¸° ì‹œì‘...")
                    time.sleep(5.0) # Playwrightì˜ ëŒ€ê¸° ê¸°ëŠ¥ ëŒ€ì‹  ë¬´ì¡°ê±´ 5ì´ˆ ëŒ€ê¸°
                    print(" Â  Â [debug] ê°•ì œ ëŒ€ê¸° ì¢…ë£Œ. íŒŒì‹± ì‹œë„.")

                    # 4. íŒŒì‹± ìˆ˜í–‰
                    detail = parse_detail(list_page)
                    href = list_page.url 

                except Exception as e:
                    error_msg = str(e).split('\n')[0]
                    print(f"[warn] í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (í´ë¦­/íŒŒì‹± ë¬¸ì œ): {error_msg}")
                    detail = {"title": title, "content": f"[ERROR] í´ë¦­ ë˜ëŠ” íŒŒì‹± ì¤‘ ì˜ˆì™¸: {error_msg}"}
                    href = "í´ë¦­ ì‹¤íŒ¨"
                
                # ìˆ˜ì§‘ëœ ì •ë³´ ì €ì¥
                collected.append({
                    "list": {"title": title, "date": date, "link": href},
                    "detail": {"title": detail["title"], "content": detail["content"]}
                })

                time.sleep(delay) 
                
        browser.close()

    # JSON ì €ì¥
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)

    print(f"\n[done] âœ¨ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected)}ê°œ í•­ëª© â†’ **{output_json}**")

if __name__ == "__main__":
    # ë”œë ˆì´ë¥¼ 1.5ì´ˆë¡œ ìœ ì§€í•˜ì—¬ ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.
    crawl(max_pages=2, delay=1.5)