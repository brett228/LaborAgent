# moel_fastcounsel_detail_scrape_final_v5.py

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
import json
import time
import re
import urllib.parse

# í¬ë¡¤ë§í•  ê¸°ë³¸ URL
BASE_URL = "https://www.moel.go.kr"
BASE_LIST_URL = "https://www.moel.go.kr/minwon/fastcounsel/fastcounselList.do"

# ìš”ì²­ í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36",
}

def parse_detail(page):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ì§ˆì˜(dt)ì™€ ë‹µë³€(dd)ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    try:
        # dl íƒœê·¸ ì°¾ê¸°
        dl_element = page.query_selector_all("dl")
        
        if not dl_element:
            body_text = page.inner_text("body")
            if "ì£„ì†¡í•©ë‹ˆë‹¤" in body_text or "ìš”ì²­ í•˜ì…¨ìŠµë‹ˆë‹¤" in body_text:
                return {"question": "[ERROR] ì ‘ê·¼ ë¶ˆê°€ í˜ì´ì§€", "answer": ""}
            return {"question": "[WARN] dl íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ", "answer": ""}
        
        # dt (ì§ˆì˜) íŒŒì‹±
        dt_element = dl_element[0].query_selector("dd")
        question = dt_element.inner_text().strip() if dt_element else "[WARN] ì§ˆì˜ ì—†ìŒ"
        
        # dd (ë‹µë³€) íŒŒì‹±
        dd_element = dl_element[1].query_selector("dd")
        answer = dd_element.inner_text().strip() if dd_element else "[WARN] ë‹µë³€ ì—†ìŒ"
        
        print('question: ', question)
        print('answer: ', answer)
        return {"question": question, "answer": answer}
        
    except Exception as e:
        return {"question": "[ERROR] íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ", "answer": str(e)}


def crawl(max_pages=3, delay=1.5, output_json="fastcounsel_with_detail.json"):
    collected = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(user_agent=HEADERS["User-Agent"], locale="ko-KR")
        list_page = context.new_page()

        print(f"ğŸš€ {BASE_LIST_URL} ì ‘ì† ì‹œë„...")
        list_page.goto(BASE_LIST_URL, timeout=45000) 
        time.sleep(delay)

        for page_index in range(1, max_pages + 1):
            print(f"\n[info] ğŸ“‹ ì²˜ë¦¬ì¤‘ ëª©ë¡ í˜ì´ì§€ {page_index}")
            
            list_page.goto(f"{BASE_LIST_URL}?pageIndex={page_index}", timeout=45000)
            time.sleep(delay)

            try:
                list_page.wait_for_selector("table tbody tr", timeout=15000)
            except PWTimeout:
                print("[warn] ëª©ë¡ì´ ë¡œë“œë˜ì§€ ì•Šì•„ í•´ë‹¹ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°.")
                continue
            
            # 1. í•­ëª© ì •ë³´ ë¯¸ë¦¬ ì¶”ì¶œ
            rows = list_page.query_selector_all("table tbody tr")
            items_to_process = []
            for tr in rows:
                qnum = tr.query_selector("td:nth-child(1)")
                a = tr.query_selector("td:nth-child(2) a")
                date_td = tr.query_selector("td:nth-child(3)")
                state_td = tr.query_selector("td:nth-child(4)")
                if a:
                    items_to_process.append({
                        "qnum": qnum.inner_text().strip(),
                        "title": a.inner_text().strip(),
                        "date": date_td.inner_text().strip(),
                        "state": state_td.inner_text().strip() if state_td else "ë¯¸í™•ì¸",
                    })
            
            # 2. ì¶”ì¶œëœ ì •ë³´ë¥¼ ìˆœíšŒí•˜ë©° ìƒì„¸ í¬ë¡¤ë§
            for item in items_to_process:
                qnum = item['qnum']
                title = item['title']
                date = item['date']
                state = item['state']
                href = "ì¸í˜ì´ì§€ í´ë¦­ ì‹¤íŒ¨"
                
                print(f"  + ìƒì„¸ ìˆ˜ì§‘ ì‹œë„ (í´ë¦­): {title}")

                try:
                    # 1. ëª©ë¡ í˜ì´ì§€ ì¬ì ‘ì† (DOM ì»¨í…ìŠ¤íŠ¸ ë³µì›)
                    list_page.goto(f"{BASE_LIST_URL}?pageIndex={page_index}", timeout=30000)
                    time.sleep(1.0) 

                    # 2. í•´ë‹¹ A íƒœê·¸ë¥¼ ì œëª© í…ìŠ¤íŠ¸ë¡œ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤.
                    a_locator = list_page.locator("td:nth-child(2) a", has_text=title).first
                    href = BASE_URL + a_locator.get_attribute('href')
                    
                    if not a_locator.is_visible():
                        print(f"    [warn] ë§í¬ ìš”ì†Œë¥¼ ë‹¤ì‹œ ì°¾ëŠ” ë° ì‹¤íŒ¨: {title}")
                        continue
                        
                    # 3. í´ë¦­ ë° ê°•ì œ ëŒ€ê¸°
                    a_locator.click(timeout=10000)
                    
                    print("    [debug] 5ì´ˆê°„ ê°•ì œ ëŒ€ê¸° ì‹œì‘...")
                    time.sleep(5.0)
                    print("    [debug] ê°•ì œ ëŒ€ê¸° ì¢…ë£Œ. íŒŒì‹± ì‹œë„.")

                    # 4. íŒŒì‹± ìˆ˜í–‰
                    detail = parse_detail(list_page)

                except Exception as e:
                    error_msg = str(e).split('\n')[0]
                    print(f"[warn] í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (í´ë¦­/íŒŒì‹± ë¬¸ì œ): {error_msg}")
                    detail = {"question": title, "answer": f"[ERROR] í´ë¦­ ë˜ëŠ” íŒŒì‹± ì¤‘ ì˜ˆì™¸: {error_msg}"}
                    href = "í´ë¦­ ì‹¤íŒ¨"
                
                # ìˆ˜ì§‘ëœ ì •ë³´ ì €ì¥
                collected.append({
                    "list": {"qnum": qnum, "title": title, "date": date, "state": state, "link": href},
                    "detail": {"question": detail["question"], "answer": detail["answer"]}
                })

                time.sleep(delay) 
                
        browser.close()

    # JSON ì €ì¥
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(collected, f, ensure_ascii=False, indent=2)

    print(f"\n[done] âœ¨ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected)}ê°œ í•­ëª© â†’ **{output_json}**")

if __name__ == "__main__":
    crawl(max_pages=1, delay=1.5)